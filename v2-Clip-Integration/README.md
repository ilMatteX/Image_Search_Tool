# ğŸ”¹ Mattex Image Search â€” CLIP Integrated Version

This version integrates **OpenAI CLIP** with **MobileNetV2** for **both visual and text-based** image retrieval.

---

## ğŸš€ Main Features

- **Dual Search Modes**:
  - ğŸ” **Visual Search**: Find similar images using MobileNetV2 features.
  - âœï¸ **Text Search**: Use natural language queries powered by CLIP.

- **Incremental Indexing**:
  - Processes only new images in two stages: MobileNet (0â€“50%) and CLIP (50â€“100%).
  - Creates and maintains separate index files: `index.npy` & `paths.txt`, `clip_index.npy` & `clip_paths.txt`.

- **Intuitive GUI (PyQt5)**:
  - Folder selection for indexing.
  - Progress bar reflecting both indexing stages.
  - Buttons for image selection and text query.
  - Category/subcategory filters.
  - Rich result items with thumbnail, score, and open-location button.

- **Automatic Backups**:
  - Saves previous indices when changes are detected in `/backup`.

---

## ğŸ“‚ Folder Structure

```
v2-Clip-Integration/
â”œâ”€â”€ main.py        # Main script with combined CLIP & MobileNet logic
â”œâ”€â”€ index.npy            # (auto) MobileNet feature vectors
â”œâ”€â”€ paths.txt            # (auto) MobileNet image paths
â”œâ”€â”€ clip_index.npy       # (auto) CLIP feature vectors
â”œâ”€â”€ clip_paths.txt       # (auto) CLIP image paths
â”œâ”€â”€ backup/              # (auto) Timestamped backups of all index files
â””â”€â”€ requirements.txt     # Python dependencies
```

---

## ğŸ”§ Installation

1. Create a Python environment (tested on 3.12)
2. Install dependencies:

```bash
pip install -r requirements.txt
```

---

## ğŸ“– Workflow

1. **Indexing**  
   - Click **â€œSelect images folderâ€** and choose your dataset root.  
   - Click **â€œStart Indexingâ€** to build/update both MobileNet and CLIP indices.  

2. **Visual Search**  
   - Click **â€œSelect image to searchâ€**, choose an image, then click **â€œSearch similar imagesâ€**.  
   - Results display top matches sorted by cosine similarity (MobileNet).

3. **Text Search**  
   - Type a descriptive query in the text field (e.g., â€œred sports car on the roadâ€).  
   - Click **â€œSearch by textâ€** to retrieve images matching the description (CLIP).

4. **Filtering & Results**  
   - Use **Category** and **Subcategory** dropdowns to narrow down results.  
   - Adjust **Results** count with the spinner.  
   - Click on thumbnails or **â€œOpen image locationâ€** for quick access.

---

## âš™ï¸ Under the Hood

- **MobileNetV2**: Extracts 1280-dimensional image vectors.
- **CLIP ViT-B/32**: Produces joint image-text embeddings.
- **Cosine similarity**: Measures closeness in feature space.
- **Progress bar**: Combines MobileNet (0â€“50%) and CLIP (50â€“100%) phases.

---

## ğŸ“ Notes

- Supported formats: `.jpg`, `.jpeg`, `.png`, `.bmp`, `.webp`.
- First run downloads CLIP model weights (~300â€¯MB).
- GPU acceleration via CUDA for PyTorch speeds up CLIP indexing.

---

## ğŸ“„ License

MIT License â€” Â© 2025 Mattex  
Feel free to use, modify, and share with credit.
